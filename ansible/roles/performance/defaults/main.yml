---
# Performance role defaults for TrueNAS SCALE
# These tunables are optimized for a server with 192GB RAM and 10GbE networking
# running 61 containerized services across 11 categories

# Enable/disable different tunable categories
performance_enable_sysctl: true
performance_enable_zfs: true
performance_enable_udev: true

# Delete and recreate strategy for tunables
performance_delete_existing: true

# SYSCTL Performance Tunables
performance_sysctl_tunables:
  # Memory Management
  - var: vm.swappiness
    value: "1"
    comment: "Minimizes swap usage with 192GB RAM available. Containers perform better with memory-resident operations."

  - var: vm.vfs_cache_pressure
    value: "50"
    comment: "Balanced inode/dentry cache retention. Default 100 is too aggressive for container workloads."

  - var: vm.dirty_ratio
    value: "10"
    comment: "Percentage of system memory that can be filled with dirty pages before forcing writes. Conservative for data integrity."

  - var: vm.dirty_background_ratio
    value: "5"
    comment: "Start background writeback at 5% dirty memory (~9.6GB with your RAM)."

  - var: vm.dirty_expire_centisecs
    value: "3000"
    comment: "Dirty pages older than 30 seconds get written. Balances performance with data safety."

  - var: vm.min_free_kbytes
    value: "1048576"
    comment: "Reserve 1GB minimum free memory for critical allocations and burst operations."

  # Shared Memory Settings
  - var: kernel.shmmax
    value: "103079215104"
    comment: "~96GB (half of total RAM) for shared memory segments used by databases in containers."

  - var: kernel.shmall
    value: "25165824"
    comment: "Total shared memory pages (96GB / 4KB page size)."

  - var: kernel.sem
    value: "250 32000 100 512"
    comment: "Semaphore parameters for container IPC: SEMMSL SEMMNS SEMOPM SEMMNI."

  # Network Buffers - Core
  - var: net.core.rmem_max
    value: "134217728"
    comment: "128MB max receive buffer for 10GbE performance."

  - var: net.core.wmem_max
    value: "134217728"
    comment: "128MB max send buffer for 10GbE performance."

  - var: net.core.rmem_default
    value: "33554432"
    comment: "32MB default receive buffer."

  - var: net.core.wmem_default
    value: "33554432"
    comment: "32MB default send buffer."

  - var: net.core.netdev_max_backlog
    value: "30000"
    comment: "Increased packet backlog for 10GbE interfaces under load."

  - var: net.core.somaxconn
    value: "8192"
    comment: "Maximum socket connection queue for container services."

  - var: net.core.netdev_budget
    value: "600"
    comment: "Packet processing budget per CPU per NAPI poll cycle for 10GbE."

  - var: net.core.netdev_budget_usecs
    value: "8000"
    comment: "Maximum time (microseconds) for packet processing per poll cycle."

  # TCP Settings
  - var: net.ipv4.tcp_rmem
    value: "4096 87380 134217728"
    comment: "TCP receive buffer: min, default, max (4KB, 85KB, 128MB)."

  - var: net.ipv4.tcp_wmem
    value: "4096 65536 134217728"
    comment: "TCP send buffer: min, default, max (4KB, 64KB, 128MB)."

  - var: net.ipv4.tcp_congestion_control
    value: "bbr"
    comment: "Google's BBR congestion control for better 10GbE utilization."

  - var: net.ipv4.tcp_mtu_probing
    value: "1"
    comment: "Enable MTU probing for jumbo frames if supported."

  - var: net.ipv4.tcp_timestamps
    value: "1"
    comment: "Required for window scaling and PAWS."

  - var: net.ipv4.tcp_sack
    value: "1"
    comment: "Selective acknowledgments for better recovery."

  - var: net.ipv4.tcp_window_scaling
    value: "1"
    comment: "Enable RFC1323 window scaling for 10GbE."

  - var: net.ipv4.tcp_no_metrics_save
    value: "1"
    comment: "Don't cache TCP metrics between connections."

  - var: net.ipv4.tcp_moderate_rcvbuf
    value: "1"
    comment: "Auto-tune receive buffers."

  - var: net.ipv4.tcp_adv_win_scale
    value: "2"
    comment: "TCP receive buffer space for application (1/2^n, so 2 = 25% for app)."

  # TCP Connection Management
  - var: net.ipv4.ip_local_port_range
    value: "32768 65000"
    comment: "Expanded ephemeral port range for containers."

  - var: net.ipv4.tcp_fin_timeout
    value: "30"
    comment: "Reduce TIME_WAIT duration from 60 to 30 seconds."

  - var: net.ipv4.tcp_keepalive_time
    value: "600"
    comment: "Start keepalive after 10 minutes of idle."

  - var: net.ipv4.tcp_keepalive_intvl
    value: "30"
    comment: "Keepalive probe interval."

  - var: net.ipv4.tcp_keepalive_probes
    value: "5"
    comment: "Number of keepalive probes before connection drop."

  - var: net.ipv4.tcp_tw_reuse
    value: "1"
    comment: "Reuse TIME_WAIT sockets for new connections."

  # Network Security
  - var: net.ipv4.conf.all.rp_filter
    value: "1"
    comment: "Reverse path filtering for security."

  - var: net.ipv4.conf.default.rp_filter
    value: "1"
    comment: "Default reverse path filtering."

  # ARP Cache Settings
  - var: net.ipv4.neigh.default.gc_thresh1
    value: "2048"
    comment: "ARP cache minimum size for container networking."

  - var: net.ipv4.neigh.default.gc_thresh2
    value: "4096"
    comment: "ARP cache soft maximum."

  - var: net.ipv4.neigh.default.gc_thresh3
    value: "8192"
    comment: "ARP cache hard maximum."

  # File System Limits
  - var: fs.file-max
    value: "2097152"
    comment: "Maximum file handles for containers and services."

  - var: fs.aio-max-nr
    value: "1048576"
    comment: "Maximum async I/O operations for databases."

  - var: fs.inotify.max_user_watches
    value: "524288"
    comment: "Inotify watches for container file monitoring."

  - var: fs.inotify.max_user_instances
    value: "8192"
    comment: "Inotify instances for containers."

# ZFS Module Parameters
performance_zfs_tunables:
  # ARC Configuration
  - var: zfs_arc_max
    value: "164987617689"
    comment: "~153GB (80% of RAM). Leaves 38GB for containers and system."

  - var: zfs_arc_min
    value: "8589934592"
    comment: "8GB minimum ARC size for consistent performance."

  - var: zfs_arc_meta_limit
    value: "82493808844"
    comment: "~76GB (50% of arc_max) for metadata caching."

  - var: zfs_arc_meta_min
    value: "4294967296"
    comment: "4GB minimum metadata cache."

  - var: zfs_arc_dnode_limit
    value: "16498761768"
    comment: "~15GB (10% of arc_max) for dnode caching."

  - var: zfs_arc_sys_free
    value: "2147483648"
    comment: "Keep 2GB RAM free for system stability."

  # Write Performance
  - var: zfs_dirty_data_max
    value: "8589934592"
    comment: "8GB max dirty data before throttling writes."

  - var: zfs_dirty_data_max_percent
    value: "10"
    comment: "10% of RAM can be dirty data."

  - var: zfs_txg_timeout
    value: "5"
    comment: "Transaction group timeout in seconds."

  # I/O Scheduling
  - var: zfs_vdev_async_read_max_active
    value: "16"
    comment: "Async read operations per vdev."

  - var: zfs_vdev_async_write_max_active
    value: "16"
    comment: "Async write operations per vdev."

  - var: zfs_vdev_sync_read_max_active
    value: "16"
    comment: "Sync read operations per vdev."

  - var: zfs_vdev_sync_write_max_active
    value: "16"
    comment: "Sync write operations per vdev."

  - var: zfs_vdev_scrub_max_active
    value: "3"
    comment: "Scrub I/O operations to minimize impact."

  - var: zfs_vdev_async_write_min_active
    value: "4"
    comment: "Minimum async writes for SSD performance."

  # I/O Aggregation
  - var: zfs_vdev_aggregation_limit
    value: "1048576"
    comment: "1MB I/O aggregation for better throughput."

  # Prefetching
  - var: zfs_prefetch_disable
    value: "0"
    comment: "Enable prefetch for sequential workloads."

  - var: zfetch_max_distance
    value: "67108864"
    comment: "64MB max distance for predictive prefetch. Balances RAM usage with sequential performance."

  # Resilver Settings
  - var: zfs_resilver_delay
    value: "2"
    comment: "Delay between resilver I/Os (milliseconds)."

  - var: zfs_resilver_min_time_ms
    value: "3000"
    comment: "Minimum resilver time per transaction group."

  - var: zfs_scan_idle
    value: "50"
    comment: "Milliseconds before scanner considers pool idle."

  # Performance Monitoring
  - var: zfs_top_maxinflight
    value: "128"
    comment: "Maximum outstanding I/O per top-level vdev."

  - var: zfs_deadman_synctime_ms
    value: "120000"
    comment: "120 second deadman timer for hung I/O detection."

  - var: zfs_deadman_ziotime_ms
    value: "300000"
    comment: "300 second timeout for individual I/Os."

  # Cache Settings
  - var: zfs_vdev_cache_size
    value: "0"
    comment: "Disable vdev cache (deprecated, ARC is sufficient)."

  # L2ARC Configuration
  - var: l2arc_write_max
    value: "134217728"
    comment: "128MB/s L2ARC write speed (if L2ARC present)."

  - var: l2arc_headroom
    value: "8"
    comment: "L2ARC headroom multiplier."

  - var: l2arc_noprefetch
    value: "0"
    comment: "Allow L2ARC prefetch."

  # Multi-host Settings
  - var: zfs_multihost_interval
    value: "1000"
    comment: "Multihost heartbeat interval (milliseconds)."

  - var: zfs_multihost_fail_intervals
    value: "10"
    comment: "Heartbeat failures before pool suspension."

  # Space Management
  - var: spa_asize_inflation
    value: "24"
    comment: "Account for metadata overhead in space calculations."

  - var: metaslab_lba_weighting_enabled
    value: "1"
    comment: "Enable LBA weighting to reduce fragmentation on HDDs by preferring lower LBAs."

  # Administration
  - var: zfs_admin_snapshot
    value: "1"
    comment: "Allow non-root snapshot administration for containers."

  - var: zfs_flags
    value: "0"
    comment: "Default ZFS module flags."

  # ZVOL Configuration
  - var: zvol_request_sync
    value: "0"
    comment: "Async zvol requests for container performance."

  - var: zvol_threads
    value: "32"
    comment: "Worker threads for zvol operations."

  - var: zvol_max_discard_blocks
    value: "16384"
    comment: "Maximum blocks per discard operation."

# UDEV Performance Tunables
# Based on original performance table specifications
performance_udev_tunables:
  # I/O scheduler settings for HDDs and SSDs
  - var: 60-scheduler-optimization
    value: |
      # Set mq-deadline scheduler for HDDs (rotational drives)
      ACTION=="add|change", KERNEL=="sd*", ATTR{queue/rotational}=="1", ATTR{queue/scheduler}="mq-deadline"
      # Set none scheduler for SSDs (non-rotational drives)
      ACTION=="add|change", KERNEL=="sd*", ATTR{queue/rotational}=="0", ATTR{queue/scheduler}="none"
      # Set none scheduler for NVMe drives
      ACTION=="add|change", KERNEL=="nvme*", ATTR{queue/scheduler}="none"
    comment: "Multi-queue deadline scheduler for rotational drives. No scheduler for NVMe/SSD, let hardware handle queuing."

  # I/O request queue depth
  - var: 61-queue-depth
    value: |
      # Set I/O request queue depth per device
      ACTION=="add|change", KERNEL=="sd*|nvme*", ATTR{queue/nr_requests}="256"
    comment: "I/O request queue depth per device. Set via /sys/block/sdX/queue/nr_requests"

  # Read-ahead optimization
  - var: 62-read-ahead
    value: |
      # 1MB read-ahead for 10GbE sequential workloads
      ACTION=="add|change", KERNEL=="sd*|nvme*", ATTR{queue/read_ahead_kb}="1024"
    comment: "1MB read-ahead for 10GbE sequential workloads. Set via /sys/block/sdX/queue/read_ahead_kb"

  # Maximum I/O size
  - var: 63-max-sectors
    value: |
      # 1MB maximum I/O size
      ACTION=="add|change", KERNEL=="sd*|nvme*", ATTR{queue/max_sectors_kb}="1024"
    comment: "1MB maximum I/O size. Set via /sys/block/sdX/queue/max_sectors_kb"

  # Rotational flag setting
  - var: 64-rotational-flag
    value: |
      # Set rotational flag based on drive type (0 for SSD, 1 for HDD)
      # Note: This is typically auto-detected, but can be overridden if needed
      ACTION=="add|change", KERNEL=="nvme*", ATTR{queue/rotational}="0"
    comment: "Set based on drive type (0 for SSD, 1 for HDD). Path: /sys/block/sdX/queue/rotational"

  # I/O merging
  - var: 65-io-merging
    value: |
      # Allow I/O merging for better throughput
      ACTION=="add|change", KERNEL=="sd*|nvme*", ATTR{queue/nomerges}="0"
    comment: "Allow I/O merging for better throughput. Set via /sys/block/sdX/queue/nomerges"

  # CPU affinity for I/O completion
  - var: 66-cpu-affinity
    value: |
      # Process I/O completions on submitting CPU
      ACTION=="add|change", KERNEL=="sd*|nvme*", ATTR{queue/rq_affinity}="2"
    comment: "Process I/O completions on submitting CPU. Set via /sys/block/sdX/queue/rq_affinity"

  # Entropy collection
  - var: 67-entropy-disable
    value: |
      # Don't use disk I/O for entropy (performance)
      ACTION=="add|change", KERNEL=="sd*|nvme*", ATTR{queue/add_random}="0"
    comment: "Don't use disk I/O for entropy (performance). Set via /sys/block/sdX/queue/add_random"

  # NVMe polling
  - var: 68-nvme-polling
    value: |
      # Enable polling for NVMe drives
      ACTION=="add|change", KERNEL=="nvme*", ATTR{queue/io_poll}="1"
      # Hybrid polling mode for NVMe
      ACTION=="add|change", KERNEL=="nvme*", ATTR{queue/io_poll_delay}="-1"
    comment: "Enable polling for NVMe drives. Set via /sys/block/nvmeXnY/queue/io_poll and io_poll_delay"

  # Write-back throttling
  - var: 69-write-throttling
    value: |
      # Write-back throttling latency target (75ms)
      ACTION=="add|change", KERNEL=="sd*|nvme*", ATTR{queue/wbt_lat_usec}="75000"
    comment: "Write-back throttling latency target (75ms). Set via /sys/block/sdX/queue/wbt_lat_usec"