TrueNAS Performance Optimization Report
Generated: {{ ansible_date_time.iso8601 }}
Host: {{ ansible_hostname }}
Configured by: Ansible Performance Role

===========================================
ZFS ARC Memory Configuration (192GB System with Containers)
===========================================
ARC Maximum Memory: {{ (zfs_arc_settings.arc_max | int / 1024 / 1024 / 1024) | round(2) }}GB (52% of total RAM - balanced for containers)
ARC Minimum Memory: {{ (zfs_arc_settings.arc_min | int / 1024 / 1024 / 1024) | round(2) }}GB (baseline with container headroom)

Memory Allocation Strategy:
- ZFS ARC: {{ (zfs_arc_settings.arc_max | int / 1024 / 1024 / 1024) | round(2) }}GB (storage cache)
- Available for containers: ~80GB (applications, databases, services)
- System overhead: ~12GB (TrueNAS, Docker, networking)

Configuration Details:
- vfs.zfs.arc_max = {{ zfs_arc_settings.arc_max }}
- vfs.zfs.arc_min = {{ zfs_arc_settings.arc_min }}

===========================================
CPU Governor Configuration
===========================================
CPU Governor: {{ cpu_governor_settings.governor }}
Hardware P-state Control: Enabled

Configuration Details:
- dev.cpu.0.freq_levels = {{ cpu_governor_settings.governor }}
- machdep.hwpstate_pkg_ctrl = 1

===========================================
Network Buffer Optimizations (4x 10GbE + 2x 1GbE)
===========================================
Network Configuration:
- 2x 10GbE interfaces for file sharing (SMB/NFS)
- 2x 10GbE interfaces for direct-connect replication
- 2x 1GbE interfaces for management/iDRAC
- Total aggregate bandwidth: ~40Gbps

Buffer Configuration:
Maximum Receive Buffer: {{ (network_buffer_settings.net_core_rmem_max | int / 1024 / 1024) | round(2) }}MB (optimized for 10GbE)
Maximum Send Buffer: {{ (network_buffer_settings.net_core_wmem_max | int / 1024 / 1024) | round(2) }}MB (optimized for 10GbE)
Default Receive Buffer: {{ (network_buffer_settings.net_core_rmem_default | int / 1024) | round(0) }}KB (high-speed default)
Default Send Buffer: {{ (network_buffer_settings.net_core_wmem_default | int / 1024) | round(0) }}KB (high-speed default)
TCP Congestion Control: {{ network_buffer_settings.net_ipv4_tcp_congestion_control }} (optimal for high-bandwidth)
Network Device Backlog: {{ network_buffer_settings.net_core_netdev_max_backlog }} (high for multiple 10GbE)

Configuration Details:
# Core buffer settings
- net.core.rmem_max = {{ network_buffer_settings.net_core_rmem_max }}
- net.core.wmem_max = {{ network_buffer_settings.net_core_wmem_max }}
- net.core.rmem_default = {{ network_buffer_settings.net_core_rmem_default }}
- net.core.wmem_default = {{ network_buffer_settings.net_core_wmem_default }}
- net.ipv4.tcp_rmem = {{ network_buffer_settings.net_ipv4_tcp_rmem }}
- net.ipv4.tcp_wmem = {{ network_buffer_settings.net_ipv4_tcp_wmem }}

# Network device settings
- net.core.netdev_max_backlog = {{ network_buffer_settings.net_core_netdev_max_backlog }}
- net.core.netdev_budget = {{ network_buffer_settings.net_core_netdev_budget }}
- net.core.netdev_budget_usecs = {{ network_buffer_settings.net_core_netdev_budget_usecs }}

# TCP optimizations for high-speed networks
- net.ipv4.tcp_congestion_control = {{ network_buffer_settings.net_ipv4_tcp_congestion_control }}
- net.ipv4.tcp_window_scaling = {{ network_buffer_settings.net_ipv4_tcp_window_scaling }}
- net.ipv4.tcp_timestamps = {{ network_buffer_settings.net_ipv4_tcp_timestamps }}
- net.ipv4.tcp_sack = {{ network_buffer_settings.net_ipv4_tcp_sack }}
- net.ipv4.tcp_mtu_probing = {{ network_buffer_settings.net_ipv4_tcp_mtu_probing }}
- net.ipv4.tcp_no_metrics_save = {{ network_buffer_settings.net_ipv4_tcp_no_metrics_save }}
- net.ipv4.tcp_adv_win_scale = {{ network_buffer_settings.net_ipv4_tcp_adv_win_scale }}

===========================================
Storage I/O Scheduler Configuration (High-Memory System)
===========================================
I/O Queue Sorting: Enabled
ZFS vdev Cache Size: 10MB
ZFS Prefetching: Enabled
ZFS TXG Timeout: 5 seconds
Queue Depth: {{ storage_io_settings.queue_depth }} (optimized for high-memory system)
Read-Ahead: {{ storage_io_settings.read_ahead_kb }}KB (larger for 10GbE file sharing)

Configuration Details:
- kern.cam.sort_io_queues = 1
- vfs.zfs.vdev.cache.size = 10485760
- vfs.zfs.prefetch_disable = 0
- vfs.zfs.txg.timeout = 5

===========================================
Performance Recommendations (192GB RAM / 40Gbps Network / Container Host)
===========================================
1. Memory Management:
   - Monitor ARC hit ratio (target >90% for container-balanced performance)
   - Watch for memory pressure between containers and ZFS ARC
   - Use 'docker stats' and 'zfs arc_summary' to monitor memory usage
   - Consider container memory limits to prevent ARC eviction
   - Reserve memory for critical containers (databases, etc.)

2. Network Optimization:
   - Enable jumbo frames (9000 MTU) on 10GbE interfaces if possible
   - Configure network interface interrupt affinity for CPU distribution
   - Monitor network utilization across all 6 interfaces
   - Consider enabling NIC offloading features (TSO, GSO, LRO) manually

3. Storage Performance:
   - Use separate ZFS datasets for different workload types
   - Enable compression (lz4/zstd) on datasets for better throughput
   - Monitor ZFS pool fragmentation and schedule regular scrubs
   - Consider using multiple vdevs for increased IOPS

4. System Monitoring:
   - Track CPU utilization during high network load
   - Monitor interrupt distribution across CPU cores
   - Watch for network buffer overruns during peak usage
   - Regular performance baselines for capacity planning

5. File Sharing Optimization:
   - Tune SMB/NFS parameters for large file transfers
   - Use dedicated networks for replication vs. client access
   - Consider block-level sync vs. file-level for replication

6. Container Management:
   - Use Docker Compose or TrueNAS Apps for container orchestration
   - Implement proper resource limits for containers:
     * Memory limits to prevent OOM conditions
     * CPU limits to prevent single container domination
   - Store container data on ZFS datasets for snapshots/replication
   - Monitor container I/O patterns and adjust ZFS recordsize if needed
   - Use container health checks and restart policies
   - Consider container placement on NUMA nodes for large workloads

===========================================
Next Steps (Container-Optimized System)
===========================================
1. Reboot the system to apply all sysctl changes
2. Monitor memory allocation between ZFS ARC and containers:
   - Check ARC stats: cat /proc/spl/kstat/zfs/arcstats
   - Monitor container memory: docker stats --no-stream
   - Watch for memory pressure: dmesg | grep -i memory
3. Deploy containers gradually and monitor resource usage
4. Adjust container memory limits based on actual usage patterns
5. Consider reducing ARC further if containers need more memory
6. Set up monitoring for both storage and container performance

Container Deployment Guidelines:
- Start with memory limits 20% below expected usage
- Monitor container restart patterns for OOM kills
- Use ZFS datasets for persistent container storage
- Enable container logging to separate ZFS datasets
- Test backup/restore procedures for container data

Note: All settings configured via TrueNAS API and persist across reboots.
Container memory usage may require ARC tuning adjustments over time.